{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w3ap-ePZFDD"
   },
   "source": [
    "# **Practical assignment for Topic 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4ab-INzTxfK"
   },
   "source": [
    "**Answer the following questions by modifying and running the code given in the lecture.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npgtHxcnZKX3"
   },
   "source": [
    "### Question 1\n",
    "According to the model `nb`, which we trained using scikit-learn library in the lecture on the email spam dataset, what is the probability of an email being a spam if the email contains just the word \"click\" 10 times without any other words present? And what if the email contains the word \"click\" 20 times? Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.603558277011681)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Vocabulary order (according to previous class): hi, dear, buy, sell, question, project, pill, click\n",
    "data = np.array([\n",
    "    [1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 0, 0, 0, 1, 1],\n",
    "    [1, 0, 0, 1, 1, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
    "    [1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
    "    [1, 0, 0, 1, 1, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 1, 0, 1, 1]])\n",
    "X = data[:, 0:8]\n",
    "y = data[:, 8]\n",
    "\n",
    "nb = BernoulliNB()\n",
    "nb.fit(X, y)\n",
    "\n",
    "click_only = np.array([[0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "spam_probability = nb.predict_proba(click_only)[0, 1]\n",
    "spam_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-kFl34KZRxo"
   },
   "source": [
    "Naive-Bayes gives a .6 spam probability when \"click\" is the only word present.\n",
    "As Bernoulli Naive Bayes only records whether a word appears at least once, repeating \"click\" 10 or 20 times triggers the same binary feature vector, so both cases lead to the same probability (60%) and the message is labeled as spam. It would be the same thing if we had the word 100 or 1000 times, while we still don't have any other word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNwPZp2wZXZl"
   },
   "source": [
    "### Question 2\n",
    "Now try the same with our own implementation given in the last cell of the notebook. You will get results that are different from those you got in question 1, which means that something is incorrect. What is the modification necessary in our code to get correct results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual model with click count 1: [0.39644172 0.60355828]\n",
      "Manual model with click count 10: [0.66335889 0.33664111]\n",
      "Manual model with click count 20: [0.66335889 0.33664111]\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes algo from the lecture notebook\n",
    "py = [np.mean(y == 0), np.mean(y == 1)]  # class priors\n",
    "alpha = 1\n",
    "\n",
    "X_manual = X.copy()\n",
    "X_manual[X_manual != 0] = 1  # ensure binary features\n",
    "m = X_manual.shape[1]\n",
    "\n",
    "for click_count in [1, 10, 20]:\n",
    "    xq = np.zeros((1, m))\n",
    "    xq[0, 7] = click_count\n",
    "\n",
    "    log_probs = []\n",
    "    for c in range(0, 2):\n",
    "        pxy = np.empty(m)\n",
    "        Ny = np.sum(y == c)\n",
    "        for j in range(0, m):\n",
    "            Nj = np.sum(X_manual[y == c, j] == xq[0, j])\n",
    "            pxy[j] = (Nj + alpha) / (Ny + alpha * 2)\n",
    "        log_p = np.log(py[c]) + np.sum(np.log(pxy))\n",
    "        log_probs.append(log_p)\n",
    "\n",
    "    probs = np.exp(log_probs)\n",
    "    probs /= np.sum(probs)\n",
    "    print(f\"Manual model with click count {click_count}: {probs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual model after binarising for click count 1: [0.39644172 0.60355828]\n",
      "Manual model after binarising for click count 10: [0.39644172 0.60355828]\n",
      "Manual model after binarising for click count 20: [0.39644172 0.60355828]\n"
     ]
    }
   ],
   "source": [
    "# Manual model after binarising the query vector like in scikit-learn\n",
    "for click_count in [1, 10, 20]:\n",
    "    xq = np.zeros((1, m))\n",
    "    xq[0, 7] = click_count\n",
    "    xq_binary = xq.copy()\n",
    "    xq_binary[xq_binary != 0] = 1\n",
    "\n",
    "    log_probs = []\n",
    "    for c in range(0, 2):\n",
    "        pxy = np.empty(m)\n",
    "        Ny = np.sum(y == c)\n",
    "        for j in range(0, m):\n",
    "            Nj = np.sum(X_manual[y == c, j] == xq_binary[0, j])\n",
    "            pxy[j] = (Nj + alpha) / (Ny + alpha * 2)\n",
    "        log_p = np.log(py[c]) + np.sum(np.log(pxy))\n",
    "        log_probs.append(log_p)\n",
    "\n",
    "    probs = np.exp(log_probs)\n",
    "    probs /= np.sum(probs)\n",
    "    print(f\"Manual model after binarising for click count {click_count}: {probs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylsb0K2PZeY6"
   },
   "source": [
    "Manual implemenation gives a different probability from scikit-learn when we repeat 'click' several times because the query vector stays numerical instead of being binary.\n",
    "As BernoulliNB transforms entries into binary, only presence / absence matters, meaning all cases still produce the same results between them.\n",
    "\n",
    "The fact that maunal model before binarising changes between 1 on one side and 10 & 20 on the other side is due to the fact that without binarisation, X_manual[y==c, j]==xq[ 0,j] treats values 10 or 20 as different of 1.\n",
    "\n",
    "To correct, we should binarise vector xq before computing probabilities and then use xq_binary in the loop.\n",
    "This modification would align manual results on those of scikit-learn and correct the unexpected change of output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWCbz49WZgBY"
   },
   "source": [
    "### Question 3\n",
    "With the email spam dataset, compute macro-averaged F1-score of the model `nb` (the one which we trained using scikit-learn library) evaluated using Leave-One-Out Cross-Validation (do not implement LOOCV yourself, just use scikit-learn functionality). What F1-score did you get? Now do the same but with Multinomial Naive Bayes. What F1-score did you get now? Which of the two algorithms would you recommend for this data? Give at least two distinct reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB LOOCV macro F1: 0.923077\n",
      "MultinomialNB LOOCV macro F1: 0.846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "bernoulli_f1 = cross_val_score(BernoulliNB(), X, y, cv=loo, scoring='f1_macro').mean()\n",
    "multinomial_f1 = cross_val_score(MultinomialNB(), X, y, cv=loo, scoring='f1_macro').mean()\n",
    "\n",
    "print(f\"BernoulliNB LOOCV macro F1: {bernoulli_f1:.6f}\")\n",
    "print(f\"MultinomialNB LOOCV macro F1: {multinomial_f1:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh5-6TUPZmEE"
   },
   "source": [
    "BernoulliNB performs better on this set. The dataset only encodes the presence / absence of words : BernoulliNB corresponds exactly to this binary way of functioning. MultinomialNB works better with counting.\n",
    "BernoulliNB is better on small datasets (here, 13 emails only), because each absent word is informative; MutinomialNB does not have this advantage as it focuses en frequences.\n",
    "With this dataset, we should use BernoulliNB, because the higher score shows us the proif that the model is more relevant with this type of binary variables. MultinomialNB could be very interesting, but we would have to change methodology and not consider features as binary, and we would need to have much more emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCbgm0K3Znqw"
   },
   "source": [
    "### Question 4\n",
    "Now go to the top of the notebook file from the lecture where we worked with the Iris dataset. Try replacing the model `nb` we used with Multinomial Naive Bayes (with appropriate import) and try to train the model. Do not modify any other code. You will get an error message. Explain the reasons behind the message specifically in the context of Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values after scaling: [-1.56757623 -1.44707648]\n",
      "MultinomialNB fitting error: Negative values in data passed to MultinomialNB (input X).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data[:, (2, 3)]\n",
    "y_iris = iris.target_names[iris.target] == 'versicolor'\n",
    "\n",
    "scaler = StandardScaler().fit(X_iris)\n",
    "X_scaled = scaler.transform(X_iris)\n",
    "print('Min values after scaling:', X_scaled.min(axis=0))\n",
    "\n",
    "try:\n",
    "    nb_multi = MultinomialNB()\n",
    "    nb_multi.fit(X_scaled, y_iris)\n",
    "except ValueError as err:\n",
    "    print('MultinomialNB fitting error:', err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWogXEWOZrOi"
   },
   "source": [
    "MultinomialNB is used when admitting that we don't have negative values. The standardization on Iris dataset produces negative value, so the algorithms rejects these."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
